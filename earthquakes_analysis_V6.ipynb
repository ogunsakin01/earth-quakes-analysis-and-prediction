{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "metadata": {
    "id": "0N3fbU037qb_"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PeF97IuyFIV",
    "outputId": "344fe5dc-f3f3-49ed-fed4-ab9871fdb14e"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# dataset path \n",
    "# eq_path = 'datasets/EQ 1965 to 2016.xlsx'\n",
    "# tectonic_Plates_path = 'datasets/Tectonic Plates Datasets.xlsx'\n",
    "eq_path = 'datasets/earth_quakes_dataset.csv'\n",
    "tectonic_Plates_path = 'datasets/tectonic_plates_dataset.csv'"
   ],
   "metadata": {
    "id": "DHhMiQ-Ryh5x"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load data in pandas dataframe \n",
    "# earthquake_data = pd.read_excel(eq_path)\n",
    "# tectonic_plate_data = pd.read_excel(tectonic_Plates_path)\n",
    "earthquake_data = pd.read_csv(eq_path)\n",
    "tectonic_plate_data = pd.read_csv(tectonic_Plates_path)"
   ],
   "metadata": {
    "id": "YpMo7tWZyHqB"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# set the display options to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "metadata": {
    "id": "EjCiuBpHZja-"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# display first 5 records of earthquake data \n",
    "earthquake_data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "j6HsQcj6yezI",
    "outputId": "a6dce1d1-eeae-4e65-8dff-15a26a0e2cc1"
   },
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "         Date      Time  Latitude  Longitude        Type  Depth  Depth Error  \\\n0  01/02/1965  13:44:18    19.246    145.616  Earthquake  131.6          NaN   \n1  01/04/1965  11:29:49     1.863    127.352  Earthquake   80.0          NaN   \n2  01/05/1965  18:05:58   -20.579   -173.972  Earthquake   20.0          NaN   \n3  01/08/1965  18:49:43   -59.076    -23.557  Earthquake   15.0          NaN   \n4  01/09/1965  13:32:50    11.938    126.427  Earthquake   15.0          NaN   \n\n   Depth Seismic Stations  Magnitude Magnitude Type  Magnitude Error  \\\n0                     NaN        6.0             MW              NaN   \n1                     NaN        5.8             MW              NaN   \n2                     NaN        6.2             MW              NaN   \n3                     NaN        5.8             MW              NaN   \n4                     NaN        5.8             MW              NaN   \n\n   Magnitude Seismic Stations  Azimuthal Gap  Horizontal Distance  \\\n0                         NaN            NaN                  NaN   \n1                         NaN            NaN                  NaN   \n2                         NaN            NaN                  NaN   \n3                         NaN            NaN                  NaN   \n4                         NaN            NaN                  NaN   \n\n   Horizontal Error  Root Mean Square            ID  Source Location Source  \\\n0               NaN               NaN  ISCGEM860706  ISCGEM          ISCGEM   \n1               NaN               NaN  ISCGEM860737  ISCGEM          ISCGEM   \n2               NaN               NaN  ISCGEM860762  ISCGEM          ISCGEM   \n3               NaN               NaN  ISCGEM860856  ISCGEM          ISCGEM   \n4               NaN               NaN  ISCGEM860890  ISCGEM          ISCGEM   \n\n  Magnitude Source     Status  \n0           ISCGEM  Automatic  \n1           ISCGEM  Automatic  \n2           ISCGEM  Automatic  \n3           ISCGEM  Automatic  \n4           ISCGEM  Automatic  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Type</th>\n      <th>Depth</th>\n      <th>Depth Error</th>\n      <th>Depth Seismic Stations</th>\n      <th>Magnitude</th>\n      <th>Magnitude Type</th>\n      <th>Magnitude Error</th>\n      <th>Magnitude Seismic Stations</th>\n      <th>Azimuthal Gap</th>\n      <th>Horizontal Distance</th>\n      <th>Horizontal Error</th>\n      <th>Root Mean Square</th>\n      <th>ID</th>\n      <th>Source</th>\n      <th>Location Source</th>\n      <th>Magnitude Source</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01/02/1965</td>\n      <td>13:44:18</td>\n      <td>19.246</td>\n      <td>145.616</td>\n      <td>Earthquake</td>\n      <td>131.6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860706</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01/04/1965</td>\n      <td>11:29:49</td>\n      <td>1.863</td>\n      <td>127.352</td>\n      <td>Earthquake</td>\n      <td>80.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.8</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860737</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01/05/1965</td>\n      <td>18:05:58</td>\n      <td>-20.579</td>\n      <td>-173.972</td>\n      <td>Earthquake</td>\n      <td>20.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.2</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860762</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01/08/1965</td>\n      <td>18:49:43</td>\n      <td>-59.076</td>\n      <td>-23.557</td>\n      <td>Earthquake</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.8</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860856</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01/09/1965</td>\n      <td>13:32:50</td>\n      <td>11.938</td>\n      <td>126.427</td>\n      <td>Earthquake</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.8</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860890</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# display first five records of tectonic plates data \n",
    "tectonic_plate_data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "29Lm_75j17Ld",
    "outputId": "5521031b-f352-4e0f-e22f-7f8422228119"
   },
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "  plate     lat      lon\n0    am  30.754  132.824\n1    am  30.970  132.965\n2    am  31.216  133.197\n3    am  31.515  133.500\n4    am  31.882  134.042",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>plate</th>\n      <th>lat</th>\n      <th>lon</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>am</td>\n      <td>30.754</td>\n      <td>132.824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>am</td>\n      <td>30.970</td>\n      <td>132.965</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>am</td>\n      <td>31.216</td>\n      <td>133.197</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>am</td>\n      <td>31.515</td>\n      <td>133.500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>am</td>\n      <td>31.882</td>\n      <td>134.042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# print total number of records in both the dataset \n",
    "print('Total records in earthquake dataset : ', earthquake_data.shape[0])\n",
    "print('Total records in tectonic plate dataset : ', earthquake_data.shape[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIiClCzQ-hDS",
    "outputId": "302ba3d5-b767-48f0-c7e2-e21711dd5078"
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in earthquake dataset :  23412\n",
      "Total records in tectonic plate dataset :  23412\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# display informaton of earthquake data \n",
    "earthquake_data.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYRPTqdA4bSy",
    "outputId": "fa44d0cb-be49-4c6f-a8fa-57737424e0ee"
   },
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23412 entries, 0 to 23411\n",
      "Data columns (total 21 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Date                        23412 non-null  object \n",
      " 1   Time                        23412 non-null  object \n",
      " 2   Latitude                    23412 non-null  float64\n",
      " 3   Longitude                   23412 non-null  float64\n",
      " 4   Type                        23412 non-null  object \n",
      " 5   Depth                       23412 non-null  float64\n",
      " 6   Depth Error                 4461 non-null   float64\n",
      " 7   Depth Seismic Stations      7097 non-null   float64\n",
      " 8   Magnitude                   23412 non-null  float64\n",
      " 9   Magnitude Type              23409 non-null  object \n",
      " 10  Magnitude Error             327 non-null    float64\n",
      " 11  Magnitude Seismic Stations  2564 non-null   float64\n",
      " 12  Azimuthal Gap               7299 non-null   float64\n",
      " 13  Horizontal Distance         1604 non-null   float64\n",
      " 14  Horizontal Error            1156 non-null   float64\n",
      " 15  Root Mean Square            17352 non-null  float64\n",
      " 16  ID                          23412 non-null  object \n",
      " 17  Source                      23412 non-null  object \n",
      " 18  Location Source             23412 non-null  object \n",
      " 19  Magnitude Source            23412 non-null  object \n",
      " 20  Status                      23412 non-null  object \n",
      "dtypes: float64(12), object(9)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tectonic_plate_data.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mSGMe0G4iwJ",
    "outputId": "02754ab2-8276-473d-fcf6-791f4e1f4d23"
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12321 entries, 0 to 12320\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   plate   12321 non-null  object \n",
      " 1   lat     12321 non-null  float64\n",
      " 2   lon     12321 non-null  float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 288.9+ KB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data PreProcessing "
   ],
   "metadata": {
    "id": "0v00Q9aIaRan"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# dropping columns with missing values\n",
    "if earthquake_data.isnull().values.any():\n",
    "    # drop any columns with missing values\n",
    "    eq_data = earthquake_data.dropna(axis=1)"
   ],
   "metadata": {
    "id": "pPPTS1po5Uwh"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# we add these 'Magnitude Type' because in this column have only three missing value \n",
    "# Group the data by 'Magnitude'\n",
    "grouped = earthquake_data.groupby('Magnitude')\n",
    "\n",
    "# Define a function to impute missing values in a series based on the most frequent value\n",
    "def impute_most_frequent(series):\n",
    "    most_frequent = series.mode().iloc[0]\n",
    "    return series.fillna(most_frequent)\n",
    "\n",
    "# Apply the imputation function to the 'Magnitude Type' column for each group\n",
    "eq_data['Magnitude Type'] = grouped['Magnitude Type'].apply(impute_most_frequent)"
   ],
   "metadata": {
    "id": "eLbJuB-RkOq2"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# display first five records\n",
    "eq_data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "P53j2TAokxqt",
    "outputId": "43ad8ac4-107a-44b0-f8af-a6dfbf3935b5"
   },
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "         Date      Time  Latitude  Longitude        Type  Depth  Magnitude  \\\n0  01/02/1965  13:44:18    19.246    145.616  Earthquake  131.6        6.0   \n1  01/04/1965  11:29:49     1.863    127.352  Earthquake   80.0        5.8   \n2  01/05/1965  18:05:58   -20.579   -173.972  Earthquake   20.0        6.2   \n3  01/08/1965  18:49:43   -59.076    -23.557  Earthquake   15.0        5.8   \n4  01/09/1965  13:32:50    11.938    126.427  Earthquake   15.0        5.8   \n\n             ID  Source Location Source Magnitude Source     Status  \n0  ISCGEM860706  ISCGEM          ISCGEM           ISCGEM  Automatic  \n1  ISCGEM860737  ISCGEM          ISCGEM           ISCGEM  Automatic  \n2  ISCGEM860762  ISCGEM          ISCGEM           ISCGEM  Automatic  \n3  ISCGEM860856  ISCGEM          ISCGEM           ISCGEM  Automatic  \n4  ISCGEM860890  ISCGEM          ISCGEM           ISCGEM  Automatic  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Type</th>\n      <th>Depth</th>\n      <th>Magnitude</th>\n      <th>ID</th>\n      <th>Source</th>\n      <th>Location Source</th>\n      <th>Magnitude Source</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01/02/1965</td>\n      <td>13:44:18</td>\n      <td>19.246</td>\n      <td>145.616</td>\n      <td>Earthquake</td>\n      <td>131.6</td>\n      <td>6.0</td>\n      <td>ISCGEM860706</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01/04/1965</td>\n      <td>11:29:49</td>\n      <td>1.863</td>\n      <td>127.352</td>\n      <td>Earthquake</td>\n      <td>80.0</td>\n      <td>5.8</td>\n      <td>ISCGEM860737</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01/05/1965</td>\n      <td>18:05:58</td>\n      <td>-20.579</td>\n      <td>-173.972</td>\n      <td>Earthquake</td>\n      <td>20.0</td>\n      <td>6.2</td>\n      <td>ISCGEM860762</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01/08/1965</td>\n      <td>18:49:43</td>\n      <td>-59.076</td>\n      <td>-23.557</td>\n      <td>Earthquake</td>\n      <td>15.0</td>\n      <td>5.8</td>\n      <td>ISCGEM860856</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01/09/1965</td>\n      <td>13:32:50</td>\n      <td>11.938</td>\n      <td>126.427</td>\n      <td>Earthquake</td>\n      <td>15.0</td>\n      <td>5.8</td>\n      <td>ISCGEM860890</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "eq_data.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyA4GTMNeitn",
    "outputId": "e10c80ad-2ef7-4ec7-99a0-06a3c74de6a6"
   },
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23412 entries, 0 to 23411\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Date              23412 non-null  object \n",
      " 1   Time              23412 non-null  object \n",
      " 2   Latitude          23412 non-null  float64\n",
      " 3   Longitude         23412 non-null  float64\n",
      " 4   Type              23412 non-null  object \n",
      " 5   Depth             23412 non-null  float64\n",
      " 6   Magnitude         23412 non-null  float64\n",
      " 7   ID                23412 non-null  object \n",
      " 8   Source            23412 non-null  object \n",
      " 9   Location Source   23412 non-null  object \n",
      " 10  Magnitude Source  23412 non-null  object \n",
      " 11  Status            23412 non-null  object \n",
      "dtypes: float64(4), object(8)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# check for incorrect dates\n",
    "try:\n",
    "    pd.to_datetime(eq_data['Date'])\n",
    "    print(\"No incorrect dates found.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUifVaS97mlZ",
    "outputId": "bd408244-cbc4-4fd9-e86e-9cb5ac7e3a05"
   },
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No incorrect dates found.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#exploring the length of date objects\n",
    "lengths = eq_data[\"Date\"].astype(str).str.len()\n",
    "lengths.value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wyPkLvYBa2I",
    "outputId": "283e6ae0-fca3-4edb-8f84-7405df595a21"
   },
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "10    23409\n24        3\nName: Date, dtype: int64"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#having a look at the fishy datapoints\n",
    "incorrect_dates = np.where([lengths == 32])[1]\n",
    "print(\"Fishy dates:\", incorrect_dates)\n",
    "eq_data.loc[incorrect_dates]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "fUJn_4n9SNaG",
    "outputId": "913fe07f-2720-4c75-d336-3a728b977098"
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fishy dates: []\n"
     ]
    },
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [Date, Time, Latitude, Longitude, Type, Depth, Magnitude, ID, Source, Location Source, Magnitude Source, Status]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Type</th>\n      <th>Depth</th>\n      <th>Magnitude</th>\n      <th>ID</th>\n      <th>Source</th>\n      <th>Location Source</th>\n      <th>Magnitude Source</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on our analysis, the \"date\" column appears to have the correct format, so no changes are needed.\n"
   ],
   "metadata": {
    "id": "XYeZjEmzDGkN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Date \n",
    "eq_data['Year'] = eq_data['Date'].dt.year\n",
    "eq_data['Month'] = eq_data['Date'].dt.month\n",
    "eq_data['Day'] = eq_data['Date'].dt.day"
   ],
   "metadata": {
    "id": "ca0HXzH5Lwpt"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "  #exploring the length of date objects\n",
    "lengths = eq_data[\"Time\"].astype(str).str.len()\n",
    "lengths.value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tRfgbBDCtS8",
    "outputId": "09c73f7d-0778-457e-870f-37b995a46291"
   },
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "8     23409\n24        3\nName: Time, dtype: int64"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Having a look at the fishy datapoints\n",
    "incorrect_time = np.where([lengths == 24])[1]\n",
    "print(\"Fishy time:\", incorrect_time)\n",
    "eq_data.loc[incorrect_time]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "NO0OzExuF9cy",
    "outputId": "b50c7c9b-4c62-4d68-f181-09d10ab20634"
   },
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fishy time: [ 3378  7512 20650]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                           Date                      Time  Latitude  \\\n3378   1975-02-23T02:58:41.000Z  1975-02-23T02:58:41.000Z     8.017   \n7512   1985-04-28T02:53:41.530Z  1985-04-28T02:53:41.530Z   -32.998   \n20650  2011-03-13T02:23:34.520Z  2011-03-13T02:23:34.520Z    36.344   \n\n       Longitude        Type  Depth  Magnitude          ID Source  \\\n3378     124.075  Earthquake  623.0        5.6  USP0000A09     US   \n7512     -71.766  Earthquake   33.0        5.6  USP0002E81     US   \n20650    142.344  Earthquake   10.1        5.8  USP000HWQP     US   \n\n      Location Source Magnitude Source    Status  \n3378               US               US  Reviewed  \n7512               US              HRV  Reviewed  \n20650              US             GCMT  Reviewed  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Type</th>\n      <th>Depth</th>\n      <th>Magnitude</th>\n      <th>ID</th>\n      <th>Source</th>\n      <th>Location Source</th>\n      <th>Magnitude Source</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3378</th>\n      <td>1975-02-23T02:58:41.000Z</td>\n      <td>1975-02-23T02:58:41.000Z</td>\n      <td>8.017</td>\n      <td>124.075</td>\n      <td>Earthquake</td>\n      <td>623.0</td>\n      <td>5.6</td>\n      <td>USP0000A09</td>\n      <td>US</td>\n      <td>US</td>\n      <td>US</td>\n      <td>Reviewed</td>\n    </tr>\n    <tr>\n      <th>7512</th>\n      <td>1985-04-28T02:53:41.530Z</td>\n      <td>1985-04-28T02:53:41.530Z</td>\n      <td>-32.998</td>\n      <td>-71.766</td>\n      <td>Earthquake</td>\n      <td>33.0</td>\n      <td>5.6</td>\n      <td>USP0002E81</td>\n      <td>US</td>\n      <td>US</td>\n      <td>HRV</td>\n      <td>Reviewed</td>\n    </tr>\n    <tr>\n      <th>20650</th>\n      <td>2011-03-13T02:23:34.520Z</td>\n      <td>2011-03-13T02:23:34.520Z</td>\n      <td>36.344</td>\n      <td>142.344</td>\n      <td>Earthquake</td>\n      <td>10.1</td>\n      <td>5.8</td>\n      <td>USP000HWQP</td>\n      <td>US</td>\n      <td>US</td>\n      <td>GCMT</td>\n      <td>Reviewed</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert Time column to string type\n",
    "eq_data['Time'] = eq_data['Time'].astype(str)"
   ],
   "metadata": {
    "id": "06MtiIY3KI6m"
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#fixing the wrong time and changing the datatype from numpy object to timedelta64[ns]\n",
    "eq_data.loc[3378, \"Time\"] = \"02:58:41\"\n",
    "eq_data.loc[7512, \"Time\"] = \"02:53:41\"\n",
    "eq_data.loc[20650, \"Time\"] = \"02:23:34\""
   ],
   "metadata": {
    "id": "Va6pqgsbGZOF"
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# convert time delta \n",
    "eq_data['Time_']= pd.to_timedelta(eq_data['Time'])"
   ],
   "metadata": {
    "id": "0J990P2Ll_BV"
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the time column to datetime format\n",
    "eq_data['Time'] = pd.to_datetime(eq_data['Time'])"
   ],
   "metadata": {
    "id": "Z6oXDb-dZJ-Q"
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract the hour, minute, and second from the time column\n",
    "eq_data['Hour'] = eq_data['Time'].dt.hour\n",
    "eq_data['Minute'] = eq_data['Time'].dt.minute\n",
    "eq_data['Second'] = eq_data['Time'].dt.second"
   ],
   "metadata": {
    "id": "zWBADkqsUSS9"
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create data and time column \n",
    "eq_data[\"Date_Time\"]=eq_data[\"Date\"] +eq_data[\"Time_\"]"
   ],
   "metadata": {
    "id": "RTU9vCK3kLKP"
   },
   "execution_count": 45,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Timedelta' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Create data and time column \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m eq_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate_Time\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m=\u001B[39m\u001B[43meq_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43meq_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTime_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/common.py:72\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m     70\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arraylike.py:102\u001B[0m, in \u001B[0;36mOpsMixin.__add__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__add__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__add__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m--> 102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_arith_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:6259\u001B[0m, in \u001B[0;36mSeries._arith_method\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m   6257\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_arith_method\u001B[39m(\u001B[38;5;28mself\u001B[39m, other, op):\n\u001B[1;32m   6258\u001B[0m     \u001B[38;5;28mself\u001B[39m, other \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39malign_method_SERIES(\u001B[38;5;28mself\u001B[39m, other)\n\u001B[0;32m-> 6259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbase\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mIndexOpsMixin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_arith_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/base.py:1325\u001B[0m, in \u001B[0;36mIndexOpsMixin._arith_method\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m   1322\u001B[0m rvalues \u001B[38;5;241m=\u001B[39m ensure_wrapped_if_datetimelike(rvalues)\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m np\u001B[38;5;241m.\u001B[39merrstate(\u001B[38;5;28mall\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1325\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marithmetic_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_result(result, name\u001B[38;5;241m=\u001B[39mres_name)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:218\u001B[0m, in \u001B[0;36marithmetic_op\u001B[0;34m(left, right, op)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;66;03m# NB: We assume that extract_array and ensure_wrapped_if_datetimelike\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;66;03m#  have already been called on `left` and `right`,\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;66;03m#  and `maybe_prepare_scalar_for_op` has already been called on `right`\u001B[39;00m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;66;03m# We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\u001B[39;00m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;66;03m# casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\u001B[39;00m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    212\u001B[0m     should_extension_dispatch(left, right)\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(right, (Timedelta, BaseOffset, Timestamp))\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# Timedelta/Timestamp and other custom scalars are included in the check\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# because numexpr will fail on it, see GH#31457\u001B[39;00m\n\u001B[0;32m--> 218\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mleft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mright\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001B[39;00m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     _bool_arith_check(op, left, right)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:2034\u001B[0m, in \u001B[0;36mTimelikeOps.__array_ufunc__\u001B[0;34m(self, ufunc, method, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2026\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2027\u001B[0m     ufunc \u001B[38;5;129;01min\u001B[39;00m [np\u001B[38;5;241m.\u001B[39misnan, np\u001B[38;5;241m.\u001B[39misinf, np\u001B[38;5;241m.\u001B[39misfinite]\n\u001B[1;32m   2028\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2029\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m inputs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   2030\u001B[0m ):\n\u001B[1;32m   2031\u001B[0m     \u001B[38;5;66;03m# numpy 1.18 changed isinf and isnan to not raise on dt64/td64\u001B[39;00m\n\u001B[1;32m   2032\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(ufunc, method)(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ndarray, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 2034\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__array_ufunc__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mufunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/base.py:1666\u001B[0m, in \u001B[0;36mExtensionArray.__array_ufunc__\u001B[0;34m(self, ufunc, method, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\n\u001B[1;32m   1662\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(other, (ABCSeries, ABCIndex, ABCDataFrame)) \u001B[38;5;28;01mfor\u001B[39;00m other \u001B[38;5;129;01min\u001B[39;00m inputs\n\u001B[1;32m   1663\u001B[0m ):\n\u001B[1;32m   1664\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[0;32m-> 1666\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43marraylike\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaybe_dispatch_ufunc_to_dunder_op\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mufunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m   1668\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[1;32m   1670\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/ops_dispatch.pyx:113\u001B[0m, in \u001B[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:1485\u001B[0m, in \u001B[0;36mDatetimeLikeArrayMixin.__radd__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__radd__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[1;32m   1484\u001B[0m     \u001B[38;5;66;03m# alias for __add__\u001B[39;00m\n\u001B[0;32m-> 1485\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__add__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/common.py:72\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m     70\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:1459\u001B[0m, in \u001B[0;36mDatetimeLikeArrayMixin.__add__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   1456\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_timedelta_arraylike(other)\n\u001B[1;32m   1457\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_object_dtype(other_dtype):\n\u001B[1;32m   1458\u001B[0m     \u001B[38;5;66;03m# e.g. Array/Index of DateOffset objects\u001B[39;00m\n\u001B[0;32m-> 1459\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_addsub_object_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1460\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_datetime64_dtype(other_dtype) \u001B[38;5;129;01mor\u001B[39;00m is_datetime64tz_dtype(other_dtype):\n\u001B[1;32m   1461\u001B[0m     \u001B[38;5;66;03m# DatetimeIndex, ndarray[datetime64]\u001B[39;00m\n\u001B[1;32m   1462\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_datetime_arraylike(other)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:1384\u001B[0m, in \u001B[0;36mDatetimeLikeArrayMixin._addsub_object_array\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m   1381\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings():\n\u001B[1;32m   1382\u001B[0m     \u001B[38;5;66;03m# filter out warnings about Timestamp.freq\u001B[39;00m\n\u001B[1;32m   1383\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mfilterwarnings(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m, category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m-> 1384\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mO\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1386\u001B[0m result \u001B[38;5;241m=\u001B[39m pd_array(res_values\u001B[38;5;241m.\u001B[39mravel())\n\u001B[1;32m   1387\u001B[0m result \u001B[38;5;241m=\u001B[39m extract_array(result, extract_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'Timedelta' and 'str'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lable Encoding "
   ],
   "metadata": {
    "id": "b-AMvqphZMMT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# define LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# encode categorical columns to numerical\n",
    "eq_data['Type'] = le.fit_transform(eq_data['Type'])\n",
    "eq_data['Source'] = le.fit_transform(eq_data['Source'])\n",
    "eq_data['Location Source'] = le.fit_transform(eq_data['Location Source'])\n",
    "eq_data['Status'] = le.fit_transform(eq_data['Status'])\n",
    "eq_data['Magnitude Type'] = le.fit_transform(eq_data['Status'])"
   ],
   "metadata": {
    "id": "Y24HhazUM-OF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# convert integer value to string \n",
    "eq_data['Magnitude Source'] = eq_data['Magnitude Source'].astype(str)"
   ],
   "metadata": {
    "id": "AOem0_rfZSrH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# label encoder \n",
    "eq_data['Magnitude Source'] = le.fit_transform(eq_data['Magnitude Source'])"
   ],
   "metadata": {
    "id": "oR_b7XCgVCxu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eq_data.head(2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "aM-vVtu8f2Qe",
    "outputId": "60ce1c41-35a0-4a3f-b852-bee337e824e9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Magnitude\" Prediction"
   ],
   "metadata": {
    "id": "PIxxZla7Eq4g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Magnitude','Time_','Date_Time'], axis=1)\n",
    "y = eq_data['Magnitude']"
   ],
   "metadata": {
    "id": "QxXC7uLdb53t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "AoEs9brEvSTN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Magnitude\" prediction"
   ],
   "metadata": {
    "id": "FqWI_XEGbcrJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDCmeSSHbb5Q",
    "outputId": "35820fad-1378-44e5-89d1-54f70180e4ae"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Magnitude\" prediction"
   ],
   "metadata": {
    "id": "vbS7_vjAFkc_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "id": "N9X2Z3xhYNJX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "181a2d26-f91e-4038-d365-77447fb0e2b1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "XK5cGffLxK9M",
    "outputId": "fcc1e46c-89f4-4b36-e6e4-abaf5bf85f2d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Location Source\" Prediction"
   ],
   "metadata": {
    "id": "_IXbAc1GJCIW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Location Source','Time_','Date_Time'], axis=1)\n",
    "y = eq_data['Location Source']"
   ],
   "metadata": {
    "id": "ziusGLYvHZfv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "gxUGwLuKLcj2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Location Source\" prediction"
   ],
   "metadata": {
    "id": "PwN5RK-QXH2z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an SVM model\n",
    "svm_model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "# Fit the model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_ctHLWnJOvf",
    "outputId": "d65408a3-280b-4a65-8707-ab63cff3c120"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Location Source\" prediction"
   ],
   "metadata": {
    "id": "vOm54LEjMbwu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "cEIGe4W7V25F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "uS-PtdEjV5mM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(1, X_train.shape[2]), activation='relu', return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(48, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = lstm_model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = lstm_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTHR25OnXp2c",
    "outputId": "37be9771-a4f3-4362-be04-a038b8de8fde"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot graph\n",
    "plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],'b',label='validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "Z4xTVADAwoPL",
    "outputId": "a793850f-8515-4350-9365-7c19c51c2eb3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Time\" Prediction "
   ],
   "metadata": {
    "id": "g2mWVsTpg8kf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#X = eq_data[['Latitude','Longitude','Depth','Magnitude']]\n",
    "X = eq_data.drop(['Date','Time','ID','Time_','Date_Time','Hour','Minute','Second','Year','Month','Day'], axis=1)\n",
    "y = pd.to_datetime(eq_data['Date_Time'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "metadata": {
    "id": "ClZGe4A_rJKI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "x2Ty7J5Nyz12"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the Timestamp objects to seconds since the Unix epoch\n",
    "y_train = y_train.astype('int64') // 10**9\n",
    "y_test = y_test.astype('int64') // 10**9"
   ],
   "metadata": {
    "id": "FSHlerAGy15I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Time\" prediction"
   ],
   "metadata": {
    "id": "pZrDHB2n043Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Compute the root mean squared error (RMSE) of the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIePpPuP0wZh",
    "outputId": "384c6288-5177-4f0e-86df-7957195375dd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Time\" prediction"
   ],
   "metadata": {
    "id": "JEzaj1GG0K5n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "gQwNDqqI1yQh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "Dt8GUiLF10T6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWuVY_Kz0Gdo",
    "outputId": "01ab1b40-90b2-475c-d50d-a0833273e101"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "JMHdfndI6ZgG",
    "outputId": "5772192c-5a31-448d-85f0-b52787927bcf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge Dataset "
   ],
   "metadata": {
    "id": "-SVQopC7Osfp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "# assume eq_data1 and eq_data2 are the two earthquake datasets\n",
    "tree = KDTree(tectonic_plate_data[['lat', 'lon']])\n",
    "\n",
    "# find the index of the closest point in eq_data2 for each point in eq_data1\n",
    "distances, indices = tree.query(eq_data[['Latitude', 'Longitude']], k=1)\n",
    "\n",
    "# merge the two datasets based on the indices\n",
    "combine_data = pd.concat([eq_data.reset_index(drop=True), tectonic_plate_data.loc[indices].reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "id": "_p5EYsTiOqny"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label encoding "
   ],
   "metadata": {
    "id": "YJSx_OPNWMvc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# encode categorical columns to numerical\n",
    "combine_data['plate'] = le.fit_transform(combine_data['plate'])"
   ],
   "metadata": {
    "id": "z4Fs2A80SnjS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Magnitude\" Prediction after Merge Dataset "
   ],
   "metadata": {
    "id": "yxxhlVQ_YZrc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Magnitude',\"Time_\",\"Date_Time\"], axis=1)\n",
    "y = eq_data['Magnitude']"
   ],
   "metadata": {
    "id": "6F3XjIm2YZCl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "vJ4pm1wMaARt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Magnitude\" prediction"
   ],
   "metadata": {
    "id": "Ip0-san6aJWT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Compute the root mean squared error (RMSE) of the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swSmmgggaLHN",
    "outputId": "97a83bc0-9563-415b-ce43-a7ea815571f4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Magnitude\" prediction after Merge Dataset "
   ],
   "metadata": {
    "id": "c_s-yAkTbNSc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWWDkqZbbXy-",
    "outputId": "cff8146e-b476-439b-f3fa-5eadd1826cbf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# plot \n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "YOx0cvesyLk0",
    "outputId": "0c4844f7-6bbe-4f75-8ea8-13bea41f09b2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Location Source\" Prediction After Merge Dataset "
   ],
   "metadata": {
    "id": "Vut-P8dzcQSN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Location Source',\"Time_\",\"Date_Time\"], axis=1)\n",
    "y = eq_data['Location Source']"
   ],
   "metadata": {
    "id": "CEpauULTcUA-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "Zeckpv1Mc6mU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Location Source\" prediction\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "AfF_Q3SvV6lt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an SVM model\n",
    "svm_model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "# Fit the model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be4SXw_BdD1M",
    "outputId": "d1cfe220-62c8-4727-e449-71200fed563f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Location Source\" prediction After Merge Dataset "
   ],
   "metadata": {
    "id": "1QQS6ys0ddu8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "7wmlPLOSdtHa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "_-7Bhn0Rd2-M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(1, X_train.shape[2]), activation='relu', return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(48, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = lstm_model.fit(X_train, y_train,  validation_split=0.2, epochs=5, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = lstm_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIstR0JEeB8t",
    "outputId": "e8c41428-a106-494d-f970-5cce9da47ab2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot graph\n",
    "plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],'b',label='validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "TViI9Yxdyi2z",
    "outputId": "52c0b9de-cfa9-4474-a119-65cfa4cdbd96"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Time\" Prediction after Merge data"
   ],
   "metadata": {
    "id": "AkS8PImr98Ot"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Time_','Date_Time','Hour','Minute','Second','Year','Month','Day'], axis=1)\n",
    "y = pd.to_datetime(eq_data['Date_Time'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "metadata": {
    "id": "E2nclB8_9Plx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "AE34u4wd9PXY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the Timestamp objects to seconds since the Unix epoch\n",
    "y_train = y_train.astype('int64') // 10**9\n",
    "y_test = y_test.astype('int64') // 10**9"
   ],
   "metadata": {
    "id": "wM54X96HDncZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Time\" prediction After Merge Data"
   ],
   "metadata": {
    "id": "6AMeF9V5Duyz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Compute the root mean squared error (RMSE) of the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0osQXLbsD1Yf",
    "outputId": "90177186-f1b6-407c-8450-05baf37d2baf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Time\" prediction after Merge data"
   ],
   "metadata": {
    "id": "gnf0rTIFD7Qb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "gYF6dyD_D3SL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "NFlsNsknEHsc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TRzHGYqEJ_-",
    "outputId": "3526a8a2-6247-4789-b58d-dd1ce5772b91"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "WbweupWSEK1T",
    "outputId": "673f75a6-9459-4366-d02b-f0de74af9f11"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a column \"Has Aftershock\" based on the information of earthquake magnitude, time and distance windows."
   ],
   "metadata": {
    "id": "yWvR7ZuDC7Uj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Set threshold and time/distance windows\n",
    "magnitude_threshold = 4.0\n",
    "time_window = pd.Timedelta(days=3)\n",
    "distance_window = 100  # in kilometers\n",
    "\n",
    "# Calculate pairwise distances between earthquakes\n",
    "distances = cdist(combine_data[['Latitude', 'Longitude']], combine_data[['lat', 'lon']])\n",
    "\n",
    "# Find the indices of earthquakes that have an aftershock within the time and distance windows\n",
    "has_aftershock = np.zeros(len(combine_data), dtype=bool)\n",
    "for i in range(len(combine_data)):\n",
    "    aftershocks = np.where((distances[i] <= distance_window) & \n",
    "                           (combine_data['Magnitude'] > magnitude_threshold) & \n",
    "                           (combine_data['Date_Time'] > combine_data.loc[i, 'Date_Time']) & \n",
    "                           (combine_data['Date_Time'] <= combine_data.loc[i, 'Date_Time'] + time_window))[0]\n",
    "    if len(aftershocks) > 0:\n",
    "        has_aftershock[i] = True\n",
    "        \n",
    "# Add a column to the dataframe indicating whether each earthquake has an aftershock or not\n",
    "combine_data['Has Aftershock'] = has_aftershock.astype(int)\n"
   ],
   "metadata": {
    "id": "dptOZVmcUiec"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a count plot\n",
    "sns.countplot(x=combine_data['Has Aftershock'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "wiUxaNBuVCVC",
    "outputId": "395a0d22-9e65-42b0-cd1a-aa7e188b3502"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Aftershock\" Prediction "
   ],
   "metadata": {
    "id": "IWOC1-ujXH-q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = combine_data.drop(['Date','Time','ID',\"Time_\",\"Date_Time\"], axis=1)\n",
    "y = combine_data['Has Aftershock']"
   ],
   "metadata": {
    "id": "uHzLBGw2XHCU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "mHP5CEQnXZ4W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Decision Tree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# svm model\n",
    "svm_model = SVC(kernel='rbf', gamma='auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)"
   ],
   "metadata": {
    "id": "MfWVOcZ3W_XF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Has Aftershock\" prediction"
   ],
   "metadata": {
    "id": "0LczfViehTdB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "va-L1LaahhmZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "ZjzQPHAAhmSS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(1, X_train.shape[2]), activation='relu', return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = lstm_model.fit(X_train, y_train,  validation_split=0.2, epochs=5, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = lstm_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cnYxqgahquR",
    "outputId": "487a22a8-8405-42c7-9dba-64b67713521f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot graph\n",
    "plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],'b',label='validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "03L4_IGP0UNt",
    "outputId": "375c9d25-56b1-4f7a-8723-c64da0af3ca4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compare the accuracies of all models\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"KNN Accuracy:\", accuracy_knn)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "print('LSTM Accuracy:', accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEHigKYnZ6AB",
    "outputId": "f72aa26e-066d-42ca-8506-5e6e7f941f67"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
