{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "metadata": {
    "id": "0N3fbU037qb_"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PeF97IuyFIV",
    "outputId": "344fe5dc-f3f3-49ed-fed4-ab9871fdb14e"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# dataset path \n",
    "# eq_path = 'datasets/EQ 1965 to 2016.xlsx'\n",
    "# tectonic_Plates_path = 'datasets/Tectonic Plates Datasets.xlsx'\n",
    "eq_path = 'datasets/earth_quakes_dataset.csv'\n",
    "tectonic_Plates_path = 'datasets/tectonic_plates_dataset.csv'"
   ],
   "metadata": {
    "id": "DHhMiQ-Ryh5x"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load data in pandas dataframe \n",
    "# earthquake_data = pd.read_excel(eq_path)\n",
    "# tectonic_plate_data = pd.read_excel(tectonic_Plates_path)\n",
    "earthquake_data = pd.read_csv(eq_path)\n",
    "tectonic_plate_data = pd.read_csv(tectonic_Plates_path)"
   ],
   "metadata": {
    "id": "YpMo7tWZyHqB"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# set the display options to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "metadata": {
    "id": "EjCiuBpHZja-",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# display first 5 records of earthquake data \n",
    "earthquake_data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "j6HsQcj6yezI",
    "outputId": "a6dce1d1-eeae-4e65-8dff-15a26a0e2cc1",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "         Date      Time  Latitude  Longitude        Type  Depth  Depth Error   \n0  01/02/1965  13:44:18    19.246    145.616  Earthquake  131.6          NaN  \\\n1  01/04/1965  11:29:49     1.863    127.352  Earthquake   80.0          NaN   \n2  01/05/1965  18:05:58   -20.579   -173.972  Earthquake   20.0          NaN   \n3  01/08/1965  18:49:43   -59.076    -23.557  Earthquake   15.0          NaN   \n4  01/09/1965  13:32:50    11.938    126.427  Earthquake   15.0          NaN   \n\n   Depth Seismic Stations  Magnitude Magnitude Type  Magnitude Error   \n0                     NaN        6.0             MW              NaN  \\\n1                     NaN        5.8             MW              NaN   \n2                     NaN        6.2             MW              NaN   \n3                     NaN        5.8             MW              NaN   \n4                     NaN        5.8             MW              NaN   \n\n   Magnitude Seismic Stations  Azimuthal Gap  Horizontal Distance   \n0                         NaN            NaN                  NaN  \\\n1                         NaN            NaN                  NaN   \n2                         NaN            NaN                  NaN   \n3                         NaN            NaN                  NaN   \n4                         NaN            NaN                  NaN   \n\n   Horizontal Error  Root Mean Square            ID  Source Location Source   \n0               NaN               NaN  ISCGEM860706  ISCGEM          ISCGEM  \\\n1               NaN               NaN  ISCGEM860737  ISCGEM          ISCGEM   \n2               NaN               NaN  ISCGEM860762  ISCGEM          ISCGEM   \n3               NaN               NaN  ISCGEM860856  ISCGEM          ISCGEM   \n4               NaN               NaN  ISCGEM860890  ISCGEM          ISCGEM   \n\n  Magnitude Source     Status  \n0           ISCGEM  Automatic  \n1           ISCGEM  Automatic  \n2           ISCGEM  Automatic  \n3           ISCGEM  Automatic  \n4           ISCGEM  Automatic  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Type</th>\n      <th>Depth</th>\n      <th>Depth Error</th>\n      <th>Depth Seismic Stations</th>\n      <th>Magnitude</th>\n      <th>Magnitude Type</th>\n      <th>Magnitude Error</th>\n      <th>Magnitude Seismic Stations</th>\n      <th>Azimuthal Gap</th>\n      <th>Horizontal Distance</th>\n      <th>Horizontal Error</th>\n      <th>Root Mean Square</th>\n      <th>ID</th>\n      <th>Source</th>\n      <th>Location Source</th>\n      <th>Magnitude Source</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01/02/1965</td>\n      <td>13:44:18</td>\n      <td>19.246</td>\n      <td>145.616</td>\n      <td>Earthquake</td>\n      <td>131.6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860706</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01/04/1965</td>\n      <td>11:29:49</td>\n      <td>1.863</td>\n      <td>127.352</td>\n      <td>Earthquake</td>\n      <td>80.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.8</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860737</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01/05/1965</td>\n      <td>18:05:58</td>\n      <td>-20.579</td>\n      <td>-173.972</td>\n      <td>Earthquake</td>\n      <td>20.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.2</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860762</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01/08/1965</td>\n      <td>18:49:43</td>\n      <td>-59.076</td>\n      <td>-23.557</td>\n      <td>Earthquake</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.8</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860856</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01/09/1965</td>\n      <td>13:32:50</td>\n      <td>11.938</td>\n      <td>126.427</td>\n      <td>Earthquake</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.8</td>\n      <td>MW</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ISCGEM860890</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>ISCGEM</td>\n      <td>Automatic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# display first five records of tectonic plates data \n",
    "tectonic_plate_data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "29Lm_75j17Ld",
    "outputId": "5521031b-f352-4e0f-e22f-7f8422228119",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  plate     lat      lon\n0    am  30.754  132.824\n1    am  30.970  132.965\n2    am  31.216  133.197\n3    am  31.515  133.500\n4    am  31.882  134.042",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>plate</th>\n      <th>lat</th>\n      <th>lon</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>am</td>\n      <td>30.754</td>\n      <td>132.824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>am</td>\n      <td>30.970</td>\n      <td>132.965</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>am</td>\n      <td>31.216</td>\n      <td>133.197</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>am</td>\n      <td>31.515</td>\n      <td>133.500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>am</td>\n      <td>31.882</td>\n      <td>134.042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# print total number of records in both the dataset \n",
    "print('Total records in earthquake dataset : ', earthquake_data.shape[0])\n",
    "print('Total records in tectonic plate dataset : ', earthquake_data.shape[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIiClCzQ-hDS",
    "outputId": "302ba3d5-b767-48f0-c7e2-e21711dd5078",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in earthquake dataset :  23412\n",
      "Total records in tectonic plate dataset :  23412\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# display informaton of earthquake data \n",
    "earthquake_data.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYRPTqdA4bSy",
    "outputId": "fa44d0cb-be49-4c6f-a8fa-57737424e0ee",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23412 entries, 0 to 23411\n",
      "Data columns (total 21 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Date                        23412 non-null  object \n",
      " 1   Time                        23412 non-null  object \n",
      " 2   Latitude                    23412 non-null  float64\n",
      " 3   Longitude                   23412 non-null  float64\n",
      " 4   Type                        23412 non-null  object \n",
      " 5   Depth                       23412 non-null  float64\n",
      " 6   Depth Error                 4461 non-null   float64\n",
      " 7   Depth Seismic Stations      7097 non-null   float64\n",
      " 8   Magnitude                   23412 non-null  float64\n",
      " 9   Magnitude Type              23409 non-null  object \n",
      " 10  Magnitude Error             327 non-null    float64\n",
      " 11  Magnitude Seismic Stations  2564 non-null   float64\n",
      " 12  Azimuthal Gap               7299 non-null   float64\n",
      " 13  Horizontal Distance         1604 non-null   float64\n",
      " 14  Horizontal Error            1156 non-null   float64\n",
      " 15  Root Mean Square            17352 non-null  float64\n",
      " 16  ID                          23412 non-null  object \n",
      " 17  Source                      23412 non-null  object \n",
      " 18  Location Source             23412 non-null  object \n",
      " 19  Magnitude Source            23412 non-null  object \n",
      " 20  Status                      23412 non-null  object \n",
      "dtypes: float64(12), object(9)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tectonic_plate_data.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mSGMe0G4iwJ",
    "outputId": "02754ab2-8276-473d-fcf6-791f4e1f4d23",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12321 entries, 0 to 12320\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   plate   12321 non-null  object \n",
      " 1   lat     12321 non-null  float64\n",
      " 2   lon     12321 non-null  float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 288.9+ KB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data PreProcessing "
   ],
   "metadata": {
    "id": "0v00Q9aIaRan"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# dropping columns with missing values\n",
    "if earthquake_data.isnull().values.any():\n",
    "    # drop any columns with missing values\n",
    "    eq_data = earthquake_data.dropna(axis=1)"
   ],
   "metadata": {
    "id": "pPPTS1po5Uwh",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# we add these 'Magnitude Type' because in this column have only three missing value \n",
    "# Group the data by 'Magnitude'\n",
    "grouped = earthquake_data.groupby('Magnitude')\n",
    "\n",
    "# Define a function to impute missing values in a series based on the most frequent value\n",
    "def impute_most_frequent(series):\n",
    "    most_frequent = series.mode().iloc[0]\n",
    "    return series.fillna(most_frequent)\n",
    "\n",
    "# Apply the imputation function to the 'Magnitude Type' column for each group\n",
    "eq_data['Magnitude Type'] = grouped['Magnitude Type'].apply(impute_most_frequent)"
   ],
   "metadata": {
    "id": "eLbJuB-RkOq2",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "incompatible index of inserted column with frame index",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/frame.py:11615\u001B[0m, in \u001B[0;36m_reindex_for_setitem\u001B[0;34m(value, index)\u001B[0m\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/series.py:4914\u001B[0m, in \u001B[0;36mSeries.reindex\u001B[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001B[0m\n\u001B[1;32m   4897\u001B[0m \u001B[38;5;129m@doc\u001B[39m(\n\u001B[1;32m   4898\u001B[0m     NDFrame\u001B[38;5;241m.\u001B[39mreindex,  \u001B[38;5;66;03m# type: ignore[has-type]\u001B[39;00m\n\u001B[1;32m   4899\u001B[0m     klass\u001B[38;5;241m=\u001B[39m_shared_doc_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mklass\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4912\u001B[0m     tolerance\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   4913\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series:\n\u001B[0;32m-> 4914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4915\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4916\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4917\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtolerance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtolerance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/generic.py:5360\u001B[0m, in \u001B[0;36mNDFrame.reindex\u001B[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001B[0m\n\u001B[1;32m   5359\u001B[0m \u001B[38;5;66;03m# perform the reindex on the axes\u001B[39;00m\n\u001B[0;32m-> 5360\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_axes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5361\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtolerance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\n\u001B[1;32m   5362\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreindex\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/generic.py:5375\u001B[0m, in \u001B[0;36mNDFrame._reindex_axes\u001B[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001B[0m\n\u001B[1;32m   5374\u001B[0m ax \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis(a)\n\u001B[0;32m-> 5375\u001B[0m new_index, indexer \u001B[38;5;241m=\u001B[39m \u001B[43max\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5376\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtolerance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtolerance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\n\u001B[1;32m   5377\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5379\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis_number(a)\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:4278\u001B[0m, in \u001B[0;36mIndex.reindex\u001B[0;34m(self, target, method, level, limit, tolerance)\u001B[0m\n\u001B[1;32m   4276\u001B[0m             indexer, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_indexer_non_unique(target)\n\u001B[0;32m-> 4278\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_reindex_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreserve_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4279\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m target, indexer\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/indexes/multi.py:2490\u001B[0m, in \u001B[0;36mMultiIndex._wrap_reindex_result\u001B[0;34m(self, target, indexer, preserve_names)\u001B[0m\n\u001B[1;32m   2489\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2490\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[43mMultiIndex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_tuples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2491\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   2492\u001B[0m     \u001B[38;5;66;03m# not all tuples, see test_constructor_dict_multiindex_reindex_flat\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/indexes/multi.py:211\u001B[0m, in \u001B[0;36mnames_compat.<locals>.new_meth\u001B[0;34m(self_or_cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mself_or_cls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/indexes/multi.py:590\u001B[0m, in \u001B[0;36mMultiIndex.from_tuples\u001B[0;34m(cls, tuples, sortorder, names)\u001B[0m\n\u001B[1;32m    588\u001B[0m         tuples \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(tuples\u001B[38;5;241m.\u001B[39m_values)\n\u001B[0;32m--> 590\u001B[0m     arrays \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtuples_to_object_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtuples\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mT)\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tuples, \u001B[38;5;28mlist\u001B[39m):\n",
      "File \u001B[0;32mpandas/_libs/lib.pyx:2894\u001B[0m, in \u001B[0;36mpandas._libs.lib.tuples_to_object_array\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Buffer dtype mismatch, expected 'Python object' but got 'long'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m series\u001B[38;5;241m.\u001B[39mfillna(most_frequent)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Apply the imputation function to the 'Magnitude Type' column for each group\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[43meq_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMagnitude Type\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m grouped[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMagnitude Type\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(impute_most_frequent)\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/frame.py:3960\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   3957\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array([key], value)\n\u001B[1;32m   3958\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3959\u001B[0m     \u001B[38;5;66;03m# set column\u001B[39;00m\n\u001B[0;32m-> 3960\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_item\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/frame.py:4153\u001B[0m, in \u001B[0;36mDataFrame._set_item\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   4143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_set_item\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, value) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   4144\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4145\u001B[0m \u001B[38;5;124;03m    Add series to DataFrame in specified column.\u001B[39;00m\n\u001B[1;32m   4146\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4151\u001B[0m \u001B[38;5;124;03m    ensure homogeneity.\u001B[39;00m\n\u001B[1;32m   4152\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4153\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sanitize_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4155\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   4156\u001B[0m         key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[1;32m   4157\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   4158\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_extension_array_dtype(value)\n\u001B[1;32m   4159\u001B[0m     ):\n\u001B[1;32m   4160\u001B[0m         \u001B[38;5;66;03m# broadcast across multiple columns if necessary\u001B[39;00m\n\u001B[1;32m   4161\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mis_unique \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns, MultiIndex):\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/frame.py:4877\u001B[0m, in \u001B[0;36mDataFrame._sanitize_column\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m   4875\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _reindex_for_setitem(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n\u001B[1;32m   4876\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_dict_like(value):\n\u001B[0;32m-> 4877\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_reindex_for_setitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_list_like(value):\n\u001B[1;32m   4880\u001B[0m     com\u001B[38;5;241m.\u001B[39mrequire_length_match(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Documents/School Work/MSc Group Project/earth-quakes-analysis-and-prediction/venv/lib/python3.11/site-packages/pandas/core/frame.py:11622\u001B[0m, in \u001B[0;36m_reindex_for_setitem\u001B[0;34m(value, index)\u001B[0m\n\u001B[1;32m  11618\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39mis_unique:\n\u001B[1;32m  11619\u001B[0m         \u001B[38;5;66;03m# duplicate axis\u001B[39;00m\n\u001B[1;32m  11620\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[0;32m> 11622\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m  11623\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincompatible index of inserted column with frame index\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m  11624\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m  11625\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m reindexed_value\n",
      "\u001B[0;31mTypeError\u001B[0m: incompatible index of inserted column with frame index"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# display first five records\n",
    "eq_data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "P53j2TAokxqt",
    "outputId": "43ad8ac4-107a-44b0-f8af-a6dfbf3935b5",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eq_data.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyA4GTMNeitn",
    "outputId": "e10c80ad-2ef7-4ec7-99a0-06a3c74de6a6",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# check for incorrect dates\n",
    "try:\n",
    "    pd.to_datetime(eq_data['Date'])\n",
    "    print(\"No incorrect dates found.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUifVaS97mlZ",
    "outputId": "bd408244-cbc4-4fd9-e86e-9cb5ac7e3a05",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#exploring the length of date objects\n",
    "lengths = eq_data[\"Date\"].astype(str).str.len()\n",
    "lengths.value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wyPkLvYBa2I",
    "outputId": "283e6ae0-fca3-4edb-8f84-7405df595a21",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#having a look at the fishy datapoints\n",
    "incorrect_dates = np.where([lengths == 32])[1]\n",
    "print(\"Fishy dates:\", incorrect_dates)\n",
    "eq_data.loc[incorrect_dates]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "fUJn_4n9SNaG",
    "outputId": "913fe07f-2720-4c75-d336-3a728b977098",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on our analysis, the \"date\" column appears to have the correct format, so no changes are needed.\n"
   ],
   "metadata": {
    "id": "XYeZjEmzDGkN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Date \n",
    "eq_data['Year'] = eq_data['Date'].dt.year\n",
    "eq_data['Month'] = eq_data['Date'].dt.month\n",
    "eq_data['Day'] = eq_data['Date'].dt.day"
   ],
   "metadata": {
    "id": "ca0HXzH5Lwpt",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "  #exploring the length of date objects\n",
    "lengths = eq_data[\"Time\"].astype(str).str.len()\n",
    "lengths.value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tRfgbBDCtS8",
    "outputId": "09c73f7d-0778-457e-870f-37b995a46291",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Having a look at the fishy datapoints\n",
    "incorrect_time = np.where([lengths == 24])[1]\n",
    "print(\"Fishy time:\", incorrect_time)\n",
    "eq_data.loc[incorrect_time]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "NO0OzExuF9cy",
    "outputId": "b50c7c9b-4c62-4d68-f181-09d10ab20634",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert Time column to string type\n",
    "eq_data['Time'] = eq_data['Time'].astype(str)"
   ],
   "metadata": {
    "id": "06MtiIY3KI6m",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#fixing the wrong time and changing the datatype from numpy object to timedelta64[ns]\n",
    "eq_data.loc[3378, \"Time\"] = \"02:58:41\"\n",
    "eq_data.loc[7512, \"Time\"] = \"02:53:41\"\n",
    "eq_data.loc[20650, \"Time\"] = \"02:23:34\""
   ],
   "metadata": {
    "id": "Va6pqgsbGZOF",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# convert time delta \n",
    "eq_data['Time_']= pd.to_timedelta(eq_data['Time'])"
   ],
   "metadata": {
    "id": "0J990P2Ll_BV",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the time column to datetime format\n",
    "eq_data['Time'] = pd.to_datetime(eq_data['Time'])"
   ],
   "metadata": {
    "id": "Z6oXDb-dZJ-Q",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract the hour, minute, and second from the time column\n",
    "eq_data['Hour'] = eq_data['Time'].dt.hour\n",
    "eq_data['Minute'] = eq_data['Time'].dt.minute\n",
    "eq_data['Second'] = eq_data['Time'].dt.second"
   ],
   "metadata": {
    "id": "zWBADkqsUSS9",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create data and time column \n",
    "eq_data[\"Date_Time\"]=eq_data[\"Date\"] +eq_data[\"Time_\"]"
   ],
   "metadata": {
    "id": "RTU9vCK3kLKP",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lable Encoding "
   ],
   "metadata": {
    "id": "b-AMvqphZMMT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# define LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# encode categorical columns to numerical\n",
    "eq_data['Type'] = le.fit_transform(eq_data['Type'])\n",
    "eq_data['Source'] = le.fit_transform(eq_data['Source'])\n",
    "eq_data['Location Source'] = le.fit_transform(eq_data['Location Source'])\n",
    "eq_data['Status'] = le.fit_transform(eq_data['Status'])\n",
    "eq_data['Magnitude Type'] = le.fit_transform(eq_data['Status'])"
   ],
   "metadata": {
    "id": "Y24HhazUM-OF",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# convert integer value to string \n",
    "eq_data['Magnitude Source'] = eq_data['Magnitude Source'].astype(str)"
   ],
   "metadata": {
    "id": "AOem0_rfZSrH",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# label encoder \n",
    "eq_data['Magnitude Source'] = le.fit_transform(eq_data['Magnitude Source'])"
   ],
   "metadata": {
    "id": "oR_b7XCgVCxu",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eq_data.head(2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "aM-vVtu8f2Qe",
    "outputId": "60ce1c41-35a0-4a3f-b852-bee337e824e9",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Magnitude\" Prediction"
   ],
   "metadata": {
    "id": "PIxxZla7Eq4g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Magnitude','Time_','Date_Time'], axis=1)\n",
    "y = eq_data['Magnitude']"
   ],
   "metadata": {
    "id": "QxXC7uLdb53t",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "AoEs9brEvSTN",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Magnitude\" prediction"
   ],
   "metadata": {
    "id": "FqWI_XEGbcrJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDCmeSSHbb5Q",
    "outputId": "35820fad-1378-44e5-89d1-54f70180e4ae",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Magnitude\" prediction"
   ],
   "metadata": {
    "id": "vbS7_vjAFkc_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "id": "N9X2Z3xhYNJX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "181a2d26-f91e-4038-d365-77447fb0e2b1",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "XK5cGffLxK9M",
    "outputId": "fcc1e46c-89f4-4b36-e6e4-abaf5bf85f2d",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Location Source\" Prediction"
   ],
   "metadata": {
    "id": "_IXbAc1GJCIW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Location Source','Time_','Date_Time'], axis=1)\n",
    "y = eq_data['Location Source']"
   ],
   "metadata": {
    "id": "ziusGLYvHZfv",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "gxUGwLuKLcj2",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Location Source\" prediction"
   ],
   "metadata": {
    "id": "PwN5RK-QXH2z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an SVM model\n",
    "svm_model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "# Fit the model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_ctHLWnJOvf",
    "outputId": "d65408a3-280b-4a65-8707-ab63cff3c120",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Location Source\" prediction"
   ],
   "metadata": {
    "id": "vOm54LEjMbwu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "cEIGe4W7V25F",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "uS-PtdEjV5mM",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(1, X_train.shape[2]), activation='relu', return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(48, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = lstm_model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = lstm_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTHR25OnXp2c",
    "outputId": "37be9771-a4f3-4362-be04-a038b8de8fde",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot graph\n",
    "plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],'b',label='validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "Z4xTVADAwoPL",
    "outputId": "a793850f-8515-4350-9365-7c19c51c2eb3",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Time\" Prediction "
   ],
   "metadata": {
    "id": "g2mWVsTpg8kf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#X = eq_data[['Latitude','Longitude','Depth','Magnitude']]\n",
    "X = eq_data.drop(['Date','Time','ID','Time_','Date_Time','Hour','Minute','Second','Year','Month','Day'], axis=1)\n",
    "y = pd.to_datetime(eq_data['Date_Time'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "metadata": {
    "id": "ClZGe4A_rJKI",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "x2Ty7J5Nyz12",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the Timestamp objects to seconds since the Unix epoch\n",
    "y_train = y_train.astype('int64') // 10**9\n",
    "y_test = y_test.astype('int64') // 10**9"
   ],
   "metadata": {
    "id": "FSHlerAGy15I",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Time\" prediction"
   ],
   "metadata": {
    "id": "pZrDHB2n043Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Compute the root mean squared error (RMSE) of the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIePpPuP0wZh",
    "outputId": "384c6288-5177-4f0e-86df-7957195375dd",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Time\" prediction"
   ],
   "metadata": {
    "id": "JEzaj1GG0K5n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "gQwNDqqI1yQh",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "Dt8GUiLF10T6",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWuVY_Kz0Gdo",
    "outputId": "01ab1b40-90b2-475c-d50d-a0833273e101",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "JMHdfndI6ZgG",
    "outputId": "5772192c-5a31-448d-85f0-b52787927bcf",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge Dataset "
   ],
   "metadata": {
    "id": "-SVQopC7Osfp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "# assume eq_data1 and eq_data2 are the two earthquake datasets\n",
    "tree = KDTree(tectonic_plate_data[['lat', 'lon']])\n",
    "\n",
    "# find the index of the closest point in eq_data2 for each point in eq_data1\n",
    "distances, indices = tree.query(eq_data[['Latitude', 'Longitude']], k=1)\n",
    "\n",
    "# merge the two datasets based on the indices\n",
    "combine_data = pd.concat([eq_data.reset_index(drop=True), tectonic_plate_data.loc[indices].reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "id": "_p5EYsTiOqny",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label encoding "
   ],
   "metadata": {
    "id": "YJSx_OPNWMvc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# encode categorical columns to numerical\n",
    "combine_data['plate'] = le.fit_transform(combine_data['plate'])"
   ],
   "metadata": {
    "id": "z4Fs2A80SnjS",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Magnitude\" Prediction after Merge Dataset "
   ],
   "metadata": {
    "id": "yxxhlVQ_YZrc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Magnitude',\"Time_\",\"Date_Time\"], axis=1)\n",
    "y = eq_data['Magnitude']"
   ],
   "metadata": {
    "id": "6F3XjIm2YZCl",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "vJ4pm1wMaARt",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Magnitude\" prediction"
   ],
   "metadata": {
    "id": "Ip0-san6aJWT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Compute the root mean squared error (RMSE) of the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swSmmgggaLHN",
    "outputId": "97a83bc0-9563-415b-ce43-a7ea815571f4",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Magnitude\" prediction after Merge Dataset "
   ],
   "metadata": {
    "id": "c_s-yAkTbNSc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWWDkqZbbXy-",
    "outputId": "cff8146e-b476-439b-f3fa-5eadd1826cbf",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# plot \n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "YOx0cvesyLk0",
    "outputId": "0c4844f7-6bbe-4f75-8ea8-13bea41f09b2",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Location Source\" Prediction After Merge Dataset "
   ],
   "metadata": {
    "id": "Vut-P8dzcQSN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Location Source',\"Time_\",\"Date_Time\"], axis=1)\n",
    "y = eq_data['Location Source']"
   ],
   "metadata": {
    "id": "CEpauULTcUA-",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "Zeckpv1Mc6mU",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Location Source\" prediction\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "AfF_Q3SvV6lt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an SVM model\n",
    "svm_model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "# Fit the model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be4SXw_BdD1M",
    "outputId": "d1cfe220-62c8-4727-e449-71200fed563f",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Location Source\" prediction After Merge Dataset "
   ],
   "metadata": {
    "id": "1QQS6ys0ddu8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "7wmlPLOSdtHa",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "_-7Bhn0Rd2-M",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(1, X_train.shape[2]), activation='relu', return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(48, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = lstm_model.fit(X_train, y_train,  validation_split=0.2, epochs=5, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = lstm_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIstR0JEeB8t",
    "outputId": "e8c41428-a106-494d-f970-5cce9da47ab2",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot graph\n",
    "plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],'b',label='validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "TViI9Yxdyi2z",
    "outputId": "52c0b9de-cfa9-4474-a119-65cfa4cdbd96",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Time\" Prediction after Merge data"
   ],
   "metadata": {
    "id": "AkS8PImr98Ot"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = eq_data.drop(['Date','Time','ID','Time_','Date_Time','Hour','Minute','Second','Year','Month','Day'], axis=1)\n",
    "y = pd.to_datetime(eq_data['Date_Time'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "metadata": {
    "id": "E2nclB8_9Plx",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "AE34u4wd9PXY",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the Timestamp objects to seconds since the Unix epoch\n",
    "y_train = y_train.astype('int64') // 10**9\n",
    "y_test = y_test.astype('int64') // 10**9"
   ],
   "metadata": {
    "id": "wM54X96HDncZ",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines (SVM) for earthquake \"Time\" prediction After Merge Data"
   ],
   "metadata": {
    "id": "6AMeF9V5Duyz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit the SVM model\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Compute the root mean squared error (RMSE) of the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0osQXLbsD1Yf",
    "outputId": "90177186-f1b6-407c-8450-05baf37d2baf",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Time\" prediction after Merge data"
   ],
   "metadata": {
    "id": "gnf0rTIFD7Qb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "gYF6dyD_D3SL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "NFlsNsknEHsc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TRzHGYqEJ_-",
    "outputId": "3526a8a2-6247-4789-b58d-dd1ce5772b91"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "WbweupWSEK1T",
    "outputId": "673f75a6-9459-4366-d02b-f0de74af9f11"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a column \"Has Aftershock\" based on the information of earthquake magnitude, time and distance windows."
   ],
   "metadata": {
    "id": "yWvR7ZuDC7Uj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Set threshold and time/distance windows\n",
    "magnitude_threshold = 4.0\n",
    "time_window = pd.Timedelta(days=3)\n",
    "distance_window = 100  # in kilometers\n",
    "\n",
    "# Calculate pairwise distances between earthquakes\n",
    "distances = cdist(combine_data[['Latitude', 'Longitude']], combine_data[['lat', 'lon']])\n",
    "\n",
    "# Find the indices of earthquakes that have an aftershock within the time and distance windows\n",
    "has_aftershock = np.zeros(len(combine_data), dtype=bool)\n",
    "for i in range(len(combine_data)):\n",
    "    aftershocks = np.where((distances[i] <= distance_window) & \n",
    "                           (combine_data['Magnitude'] > magnitude_threshold) & \n",
    "                           (combine_data['Date_Time'] > combine_data.loc[i, 'Date_Time']) & \n",
    "                           (combine_data['Date_Time'] <= combine_data.loc[i, 'Date_Time'] + time_window))[0]\n",
    "    if len(aftershocks) > 0:\n",
    "        has_aftershock[i] = True\n",
    "        \n",
    "# Add a column to the dataframe indicating whether each earthquake has an aftershock or not\n",
    "combine_data['Has Aftershock'] = has_aftershock.astype(int)\n"
   ],
   "metadata": {
    "id": "dptOZVmcUiec"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a count plot\n",
    "sns.countplot(x=combine_data['Has Aftershock'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "wiUxaNBuVCVC",
    "outputId": "395a0d22-9e65-42b0-cd1a-aa7e188b3502"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split for \"Aftershock\" Prediction "
   ],
   "metadata": {
    "id": "IWOC1-ujXH-q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = combine_data.drop(['Date','Time','ID',\"Time_\",\"Date_Time\"], axis=1)\n",
    "y = combine_data['Has Aftershock']"
   ],
   "metadata": {
    "id": "uHzLBGw2XHCU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "mHP5CEQnXZ4W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Decision Tree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# svm model\n",
    "svm_model = SVC(kernel='rbf', gamma='auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)"
   ],
   "metadata": {
    "id": "MfWVOcZ3W_XF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Short-Term Memory (LSTM) for earthquake \"Has Aftershock\" prediction"
   ],
   "metadata": {
    "id": "0LczfViehTdB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "va-L1LaahhmZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape the data for LSTM\n",
    "# Reshape the feature data to be 3-dimensional\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ],
   "metadata": {
    "id": "ZjzQPHAAhmSS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize an LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(1, X_train.shape[2]), activation='relu', return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = lstm_model.fit(X_train, y_train,  validation_split=0.2, epochs=5, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = lstm_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cnYxqgahquR",
    "outputId": "487a22a8-8405-42c7-9dba-64b67713521f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot graph\n",
    "plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],'b',label='validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "03L4_IGP0UNt",
    "outputId": "375c9d25-56b1-4f7a-8723-c64da0af3ca4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compare the accuracies of all models\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"KNN Accuracy:\", accuracy_knn)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "print('LSTM Accuracy:', accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEHigKYnZ6AB",
    "outputId": "f72aa26e-066d-42ca-8506-5e6e7f941f67"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
